{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import polars as pl\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cpu'\n",
    "sys.path.append('/home/sebacastillo/neuralnets/')\n",
    "from src.utils import get_project_root\n",
    "root = get_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "OR_problem = False\n",
    "if OR_problem:\n",
    "   df = np.loadtxt(str(root) + '/data/OR.csv',\n",
    "                 delimiter=\",\")\n",
    "else:\n",
    "   df = np.loadtxt(str(root) + '/data/XOR.csv',\n",
    "                 delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentron original (Rosenblatt: 1962)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliograf√≠a y fuentes:   \n",
    "- [Introduction to Machine Learning](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(df)\n",
    "\n",
    "def ttsplit(data, prop=0.2):\n",
    "    assert isinstance(data, np.ndarray), \"Input should be a NumPy array\"\n",
    "    p = int(len(data) * prop)\n",
    "    test = data[:p, :]\n",
    "    train = data[p:, :]\n",
    "    X_train = train[:,:-1]\n",
    "    y_train = train[:,-1]\n",
    "    X_test = test[:,:-1]\n",
    "    y_test = test[:,-1]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = ttsplit(df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (theta): [ 0.01403 -0.30108]\n",
      "Bias (theta_0): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Data imputs\n",
    "input_data = X_train\n",
    "true_labels = y_train\n",
    "\n",
    "# Initialize the weights and bias\n",
    "n = input_data.shape[0]\n",
    "weights = np.ones(input_data.shape[1])*0.5\n",
    "bias = 0\n",
    "\n",
    "# Number of epochs (tau)\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i in range(n):\n",
    "        \n",
    "        x = input_data[i]\n",
    "        y_true = true_labels[i]\n",
    "\n",
    "        # Calculate the output using weights and bias. These is our hipothesis.        \n",
    "        # the sign mater for the next conditional product.\n",
    "        # The first output is always wrong! because all weight are set to 0\n",
    "        output = np.dot(weights, x) + bias\n",
    "\n",
    "        # Sign Function part where we compare the sign of the true label and the sign of our prediction.\n",
    "        # If both y_true and output have the same sign the result is positive and the condition fail.\n",
    "        # If it never fail it means the we hold the hipotesis (weight and bias).\n",
    "        # If both have diferent sign the result of the product is negative and the codition is true.\n",
    "        # So we should update the weights.\n",
    "        if y_true * output <= 0:\n",
    "            # Here is where learning happens as weight update\n",
    "            weights += y_true * x\n",
    "            bias += y_true\n",
    "\n",
    "# The final weights (theta) and bias (theta_0)\n",
    "print(\"Weights (theta):\", weights)\n",
    "print(\"Bias (theta_0):\", bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp.1:Acuracy: .76\n",
    "#Weights (theta): [1.98073 1.32935]\n",
    "#Bias (theta_0): 1.0\n",
    "# Exp.2: Acuracy: .73\n",
    "#Weights (theta): [-0.17513 -1.07761]\n",
    "#Bias (theta_0): 1.0\n",
    "# Exp.3: Acuracy: .71\n",
    "#Weights (theta): [1.39779 1.20869]\n",
    "#Bias (theta_0): 1.0\n",
    "# Exp.4 con 200 epoch: Acuracy: 51: Bajo!\n",
    "#Weights (theta): [-0.37849 -0.58772]\n",
    "#Bias (theta_0): 0.0\n",
    "# muchos con bajo accuracy\n",
    "# Exp.muchos+1: Accuracy .74\n",
    "#Weights (theta): [ 1.5557  -1.07645]\n",
    "#Bias (theta_0): -1.0\n",
    "\n",
    "\n",
    "def validate_perceptron(X_test, y_test, weights, bias):\n",
    "    total_error = 0\n",
    "    correct_predictions = 0\n",
    "    n = X_test.shape[0]\n",
    "\n",
    "    for i in range(n):\n",
    "        x = X_test[i]\n",
    "        y_true = y_test[i]\n",
    "\n",
    "        # Calculate the output using weights and bias\n",
    "        output = np.dot(weights, x) + bias\n",
    "        y_pred = np.sign(output) if output != 0 else 1\n",
    "\n",
    "        # Calculate the L2 loss\n",
    "        error = (y_true - y_pred) ** 2\n",
    "        total_error += error\n",
    "\n",
    "        # Update the accuracy counter\n",
    "        if y_pred == y_true:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # Calculate average error and accuracy\n",
    "    average_error = total_error / n\n",
    "    accuracy = correct_predictions / n\n",
    "\n",
    "    return average_error, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average L2 Loss on Test Data: 2.0\n",
      "Accuracy on Test Data: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Validate the perceptron model\n",
    "average_error, accuracy = validate_perceptron(X_test, y_test, weights, bias)\n",
    "\n",
    "print(\"Average L2 Loss on Test Data:\", average_error)\n",
    "print(\"Accuracy on Test Data:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron con Descenso Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.0252268 0.0183228]\n",
      "Bias: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_gradient(y_true, y_pred, x):\n",
    "    gradient_w = -2 * (y_true - y_pred) * x\n",
    "    gradient_b = -2 * (y_true - y_pred)\n",
    "    return gradient_w, gradient_b\n",
    "\n",
    "def train_gradient_descent(input_data, true_labels, epochs, learning_rate):\n",
    "    n = input_data.shape[0]\n",
    "    weights = np.zeros(input_data.shape[1])\n",
    "    bias = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n):\n",
    "            x = input_data[i]\n",
    "            y_true = true_labels[i]\n",
    "\n",
    "            # Calculate the output using weights and bias\n",
    "            output = np.dot(weights, x) + bias\n",
    "            y_pred = np.sign(output) if output != 0 else -1\n",
    "\n",
    "            # Compute the gradient of the loss\n",
    "            gradient_w, gradient_b = compute_gradient(y_true, y_pred, x)\n",
    "\n",
    "            # Update the weights and bias\n",
    "            weights -= learning_rate * gradient_w\n",
    "            bias -= learning_rate * gradient_b\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Train the gradient descent-based perceptron model\n",
    "epochs = 500\n",
    "learning_rate = 0.1\n",
    "weights, bias = train_gradient_descent(X_train, y_train, epochs, learning_rate)\n",
    "\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average L2 Loss on Test Data: 1.990909090909091\n",
      "Accuracy on Test Data: 0.5022727272727273\n"
     ]
    }
   ],
   "source": [
    "def validate_perceptron(X_test, y_test, weights, bias):\n",
    "    total_error = 0\n",
    "    correct_predictions = 0\n",
    "    n = X_test.shape[0]\n",
    "\n",
    "    for i in range(n):\n",
    "        x = X_test[i]\n",
    "        y_true = y_test[i]\n",
    "\n",
    "        # Calculate the output using weights and bias\n",
    "        output = np.dot(weights, x) + bias\n",
    "        y_pred = np.sign(output) if output != 0 else 1\n",
    "\n",
    "        # Calculate the L2 loss\n",
    "        error = (y_true - y_pred) ** 2\n",
    "        total_error += error\n",
    "\n",
    "        # Update the accuracy counter\n",
    "        if y_pred == y_true:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # Calculate average error and accuracy\n",
    "    average_error = total_error / n\n",
    "    accuracy = correct_predictions / n\n",
    "\n",
    "    return average_error, accuracy\n",
    "\n",
    "# Validate the perceptron model\n",
    "average_error, accuracy = validate_perceptron(X_test, y_test, weights, bias)\n",
    "\n",
    "print(\"Average L2 Loss on Test Data:\", average_error)\n",
    "print(\"Accuracy on Test Data:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teorema de convergencia del perceptron   \n",
    "\n",
    "Convergence theorem\n",
    "The basic result about the perceptron is that, if the training data  Dn  is linearly separable, then the perceptron algorithm is guaranteed to find a linear separator.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preguntas:    \n",
    "\n",
    "- porqu√© inicializamos weights en 0.5?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo funcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar(num_features):\n",
    "    #np.random.randn(num_features)\n",
    "    w = np.concatenate(([0.0], np.ones(num_features) * 0.5))    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.5, 0.5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = inicializar(2)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de Activaci√≥n\n",
    "def sigmoid(z): \n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    \"\"\" \n",
    "    return 1 / (1 + np.exp(-z))    \n",
    "\n",
    "def sign(x, weights, bias):\n",
    "    \"\"\"\n",
    "    Sign activation function    \n",
    "    \"\"\" \n",
    "    # Calculate the weighted sum of the input features and bias\n",
    "    weighted_sum = np.dot(weights, x) + bias\n",
    "    # Apply the sign function to the weighted sum\n",
    "    if weighted_sum >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, weights, bias, y_true):\n",
    "    y_pred = sign(x, weights, bias)\n",
    "    error = (y_true - y_pred) ** 2\n",
    "    return y_pred, error\n",
    "\n",
    "def backward_pass(x, y_true, y_pred, weights, bias, learning_rate):\n",
    "    gradient_w = -2 * (y_true - y_pred) * x\n",
    "    gradient_b = -2 * (y_true - y_pred)\n",
    "    \n",
    "    weights -= learning_rate * gradient_w\n",
    "    bias -= learning_rate * gradient_b\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Iterate over the input data\n",
    "for i in range(X_train.shape[0]):\n",
    "    x = X_train[i]\n",
    "    y_true = y_train[i]\n",
    "    weights = w[1:]\n",
    "    bias = w[0]\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred, error = forward_pass(x, weights, bias, y_true)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "       print(f\"Observation {i + 1}: Predicted Output: {y_pred}, L2 Loss: {error}\")\n",
    "\n",
    "    # Backward pass\n",
    "    weights, bias = backward_pass(x, y_true, y_pred, weights, bias, learning_rate)\n",
    "\n",
    "print(\"Final Weights:\", weights)\n",
    "print(\"Final Bias:\", bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize error and accuracy counters\n",
    "total_error = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    x = X_test[i]\n",
    "    y_true = y_test[i]\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred, error = forward_pass(x, weights, bias, y_true)\n",
    "\n",
    "    # Accumulate error and update accuracy counter\n",
    "    total_error += error\n",
    "    if y_pred == y_true:\n",
    "        correct_predictions += 1   \n",
    "\n",
    "# Calculate average error and accuracy\n",
    "average_error = total_error / X_test.shape[0]\n",
    "accuracy = correct_predictions / X_test.shape[0]\n",
    "\n",
    "print(\"Average L2 Loss on Test Data:\", average_error)\n",
    "print(\"Accuracy on Test Data:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".neuralnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
