{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "# NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "from matplotlib.colors import to_rgba\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "# Path\n",
    "import sys\n",
    "sys.path.append('/home/sebacastillo/neuralnets/')\n",
    "from src.utils import get_project_root\n",
    "root = get_project_root()\n",
    "## Check torch version\n",
    "print(f'Using {torch.__version__}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else torch.device('cpu'))\n",
    "torch.manual_seed(42)\n",
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"momentum\": 0.6,\n",
    "    \"acc\": 0.0,\n",
    "    \"epoca\": 0,\n",
    "    \"input_file\": '/data/XOR.csv',\n",
    "    \"EXP_NAME\": 'EXP009',\n",
    "    \"MIN_ACC\": 1.0,\n",
    "    \"MIN_ERROR\": 1E6,\n",
    "    \"MAX_EPOCAS\": 1000,\n",
    "    \"MAX_COUNTER\": 50,\n",
    "    \"BATCH_SIZE\": 68\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by Step Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_save_data(input_filename, output_name='EXP', split_type='train_test', train_ratio=0.75, validate_ratio=None, test_ratio=None):\n",
    "\n",
    "    data = pd.read_csv(input_filename)\n",
    "\n",
    "    # Check if 'exp' folder exists, create it if it doesn't\n",
    "    if not os.path.exists('exp'):\n",
    "        os.makedirs('exp')\n",
    "    \n",
    "    # Create a subfolder with the output_name\n",
    "    output_path = os.path.join('exp', output_name)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    if split_type == 'train_validate_test':\n",
    "        if not validate_ratio or not test_ratio:\n",
    "            raise ValueError(\"Please provide validate_ratio and test_ratio for 'train_validate_test' split type.\")\n",
    "        \n",
    "        train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "        validate_data, test_data = train_test_split(temp_data, train_size=validate_ratio / (validate_ratio + test_ratio), random_state=42)\n",
    "        \n",
    "        # Save the train, validate, and test data as CSV files in the output folder\n",
    "        train_data.to_csv(os.path.join(output_path, f'{output_name}_train_data.csv'), index=False)\n",
    "        validate_data.to_csv(os.path.join(output_path, f'{output_name}_validate_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(output_path, f'{output_name}_test_data.csv'), index=False)\n",
    "\n",
    "\n",
    "        return train_data, validate_data, test_data    \n",
    "\n",
    "    elif split_type == 'train_test':\n",
    "        train_data, test_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "        \n",
    "        # Save the train and test data as CSV files in the output folder\n",
    "        train_data.to_csv(os.path.join(output_path, f'{output_name}_train_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(output_path, f'{output_name}_test_data.csv'), index=False)\n",
    "\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid split_type. Use either 'train_validate_test' or 'train_test'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_split_save_data((str(root) + '/data/XOR.csv'), 'EXP010', split_type='train_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA(Dataset):\n",
    "    '''\n",
    "    Lee los datos \n",
    "    Alimenta modelos\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        data = data.to_numpy()\n",
    "        self.X = data[:,:-1].astype(np.float32)\n",
    "        y = data[:, -1].astype(np.float32)\n",
    "        # Relabel class -1 to 0\n",
    "        y[y== -1] = 0\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        # devuelve numero observacines data\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # devulve observaciones indexadas        \n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DATA(train)\n",
    "test_data = DATA(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=params['BATCH_SIZE'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=params['BATCH_SIZE'], shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, n_inputs, n_outputs):\n",
    "        \n",
    "        super().__init__() # ejecuta init en nn.Module\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.layer1 = nn.Linear(self.n_features, self.n_inputs, bias=True)\n",
    "        self.layer2 = nn.Linear(self.n_inputs, self.n_outputs, bias=True)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        y_pred = self.layer1(X)\n",
    "        y_pred = self.tanh(y_pred)\n",
    "        y_pred = self.layer2(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data, loss_function, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    N_batches = len(data)\n",
    "\n",
    "    error = 0\n",
    "\n",
    "    for idx,(X,y) in enumerate(data):\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # volvemos a 0 weights\n",
    "\n",
    "        y_pred = model(X)\n",
    "\n",
    "        if (data.batch_size == 1):\n",
    "            loss = loss_function(y_pred.squeeze(), y.squeeze())\n",
    "        else:\n",
    "            loss = loss_function(y_pred.squeeze(), y)\n",
    "    \n",
    "        error += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    error /= N_batches\n",
    "\n",
    "    return error, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(model, data, loss_function, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    N_batches = len(data)\n",
    "\n",
    "    error = 0\n",
    "\n",
    "    Y = torch.tensor([])\n",
    "    Yp = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for idx, (X,y) in enumerate(data):\n",
    "\n",
    "            Y = torch.hstack((Y, y.flatten()))\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            Yp = torch.hstack((Yp, y_pred.flatten().cpu()))\n",
    "\n",
    "            loss = loss_function(y_pred.squeeze(), y.squeeze())\n",
    "\n",
    "            error += loss.item()\n",
    "    \n",
    "    error /= N_batches\n",
    "\n",
    "    return error, Y, Yp         \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================\n",
    "# Inicializamos el modelo\n",
    "#=============================================\n",
    "modelo = MODEL(n_features=2, n_inputs=3, n_outputs=1)\n",
    "#=============================================\n",
    "# Definimos la función de LOSS a utilizar\n",
    "#=============================================\n",
    "loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "#=============================================\n",
    "# Definimos el optimizador a utilizar\n",
    "# >>> 3er paso del bacpropagation\n",
    "#=============================================\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=params['learning_rate'], momentum=0.9)\n",
    "#optimizer = optim.Adam(modelo.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []  # Inicializo estructura para almacenar  los errores en el tiempo\n",
    "accuracy = []  # Inicializo estructura para almacenar  el accuracy en el tiempo\n",
    "STOP = False\n",
    "counter = 0\n",
    "best_model = None\n",
    "best_model_weights = None\n",
    "epoca = params['epoca']\n",
    "acc = params['acc']\n",
    "\n",
    "#===============================================================\n",
    "while (epoca < params['MAX_EPOCAS']) and (acc < params['MIN_ACC']) and (not STOP):\n",
    "\n",
    "    epoca += 1\n",
    "    \n",
    "    #----------------------\n",
    "    # ENTRENAMIENTO\n",
    "    #----------------------\n",
    "    _,modelo = train_step(modelo, train_loader, loss_function, optimizer, device)\n",
    "    \n",
    "    #----------------------\n",
    "    # VALIDACION\n",
    "    #----------------------\n",
    "    e,Y,Yp = predict_step(modelo, test_loader, loss_function, device)\n",
    "    \n",
    "    # TRANSFORMO SALIDA EN {0,1}\n",
    "    Y_pred = torch.sigmoid(Yp)\n",
    "    Y_pred[Y_pred < 0.5] = 0\n",
    "    Y_pred[Y_pred > 0] = 1\n",
    "    acc = torch.sum(Y_pred == Y)/ len(Y)\n",
    "    \n",
    "    #----------------------\n",
    "    # ALMACENO MEDIDAS\n",
    "    #----------------------\n",
    "    error.append(e)\n",
    "    accuracy.append(acc)\n",
    "    \n",
    "    #-----------------------------------------------\n",
    "    # CRITERIO DE CORTE Y ALMACENAMIENTO DEL MODELO\n",
    "    #-----------------------------------------------\n",
    "    if (e < params['MIN_ERROR']):\n",
    "        params['MIN_ERROR'] = e\n",
    "        counter = 0\n",
    "        \n",
    "        #·······················\n",
    "        # Almaceno el modelo\n",
    "        #·······················\n",
    "        best_model = deepcopy(modelo)  # Genero una copia independiente\n",
    "        best_model_weights = best_model.state_dict()\n",
    "        \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter > params['MAX_COUNTER']:\n",
    "            STOP = True\n",
    "    \n",
    "    #--------------------------------------------\n",
    "    # MUESTRO REPORTE POR PANTALLA (POR EPOCA)\n",
    "    #--------------------------------------------\n",
    "    if (epoca % 10) == 0:\n",
    "        print(f'Epoca: {epoca} -- Error: {e:.4}\\t--\\tTasa acierto [train]: {acc}\\n')\n",
    "#===============================================================\n",
    "\n",
    "#--------------------------------------------\n",
    "# MUESTRO REPORTE POR PANTALLA (FINAL)\n",
    "#--------------------------------------------\n",
    "print('='*79)\n",
    "print(f'FINAL -- Epoca: {epoca} -- Error: {e:.4}\\t--\\tTasa acierto [train]: {acc:.4}')\n",
    "print('='*79)\n",
    "\n",
    "#-----------------------------\n",
    "# GUARDO MEJOR MODELO A DISCO\n",
    "#-----------------------------\n",
    "torch.save(best_model,\n",
    "           'best_model.pt',\n",
    "           _use_new_zipfile_serialization=True)\n",
    "        \n",
    "#----------------------------------------------\n",
    "# GUARDAMOS LOS PESOS DEL MEJOR MODELO A DISCO\n",
    "#----------------------------------------------\n",
    "torch.save(best_model.state_dict(),\n",
    "           'best_model_state_dict.pt',\n",
    "           _use_new_zipfile_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".neuralnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
