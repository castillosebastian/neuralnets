{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full NN Build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup general del experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"acc\": 0.0,\n",
    "    \"epoca\": 0,\n",
    "    \"input_file\": '/data/concentlite.csv',\n",
    "    \"EXP_NAME\": 'EXP009',\n",
    "    \"MIN_ACC\": 1.0,\n",
    "    \"MIN_ERROR\": 1E6,\n",
    "    \"MAX_EPOCAS\": 1000,\n",
    "    \"MAX_COUNTER\": 50,\n",
    "    \"BATCH_SIZE\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "# NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "from matplotlib.colors import to_rgba\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "# Path\n",
    "import sys\n",
    "sys.path.append('/home/sebacastillo/neuralnets/')\n",
    "from src.utils import get_project_root\n",
    "root = get_project_root()\n",
    "## Check torch version\n",
    "print(f'Using {torch.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else torch.device('cpu'))\n",
    "torch.manual_seed(42)\n",
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_save_data(input_filename, output_name='EXP', split_type='train_test', train_ratio=0.75, validate_ratio=None, test_ratio=None):\n",
    "\n",
    "    data = pd.read_csv(input_filename)\n",
    "\n",
    "    # Check if 'exp' folder exists, create it if it doesn't\n",
    "    if not os.path.exists('exp'):\n",
    "        os.makedirs('exp')\n",
    "    \n",
    "    # Create a subfolder with the output_name\n",
    "    output_path = os.path.join('exp', output_name)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    if split_type == 'train_validate_test':\n",
    "        if not validate_ratio or not test_ratio:\n",
    "            raise ValueError(\"Please provide validate_ratio and test_ratio for 'train_validate_test' split type.\")\n",
    "        \n",
    "        train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "        validate_data, test_data = train_test_split(temp_data, train_size=validate_ratio / (validate_ratio + test_ratio), random_state=42)\n",
    "        \n",
    "        # Save the train, validate, and test data as CSV files in the output folder\n",
    "        train_data.to_csv(os.path.join(output_path, f'{output_name}_train_data.csv'), index=False)\n",
    "        validate_data.to_csv(os.path.join(output_path, f'{output_name}_validate_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(output_path, f'{output_name}_test_data.csv'), index=False)\n",
    "\n",
    "\n",
    "        return train_data, validate_data, test_data    \n",
    "\n",
    "    elif split_type == 'train_test':\n",
    "        train_data, test_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "        \n",
    "        # Save the train and test data as CSV files in the output folder\n",
    "        train_data.to_csv(os.path.join(output_path, f'{output_name}_train_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(output_path, f'{output_name}_test_data.csv'), index=False)\n",
    "\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid split_type. Use either 'train_validate_test' or 'train_test'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATASET(Dataset):  \n",
    "    '''\n",
    "    Esta clase maneja la lectura de los datos y provee un mecanismo\n",
    "    para alimentar los modelos con los patrones.\n",
    "    '''\n",
    "    \n",
    "    #===================================================\n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        #------------------------------------\n",
    "        # LECTURA DE LOS DATOS\n",
    "        data = pd.read_csv(filename, header=None).to_numpy() # Levanta los datos en formato numpy\n",
    "        \n",
    "        #------------------------------------\n",
    "        # INSERTAMOS COLUMNA DEL \"BIAS\"\n",
    "        #bias = -np.ones((len(data), 1))\n",
    "        #data = np.concatenate((bias, data), axis=1)  # Insertamos el \"bias\" en la primera columna\n",
    "        \n",
    "        #------------------------------------\n",
    "        # ALEATORIZO LOS PATRONES (filas)\n",
    "        idxs = np.arange(len(data))  # Genero un vector de índices\n",
    "        np.random.shuffle(idxs)\n",
    "        data = data[idxs,:]\n",
    "        \n",
    "        #------------------------------------\n",
    "        # SEPARO LOS DATOS\n",
    "        self.x = data[:,:-1].astype(np.float32)\n",
    "        self.y = data[:,-1].astype(np.float32)  # La clase está en la última columna\n",
    "    \n",
    "    #===================================================\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Devuelve el número de patrones en el dataset.\n",
    "        '''\n",
    "        return len(self.x)\n",
    "    \n",
    "    \n",
    "    #===================================================\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Devuelve el/los patrones indicados.\n",
    "        '''\n",
    "        return self.x[idx,:], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_with_labels(data):\n",
    "    # Filter data by label\n",
    "    data_label_1 = data[data[:, -1] == 1][:, 0:2]\n",
    "    data_label_minus_1 = data[data[:, -1] == -1][:, 0:2]\n",
    "\n",
    "    # Create scatter plots for each label\n",
    "    plt.scatter(data_label_1[:, 0], data_label_1[:, 1], label='1', alpha=0.5)\n",
    "    plt.scatter(data_label_minus_1[:, 0], data_label_minus_1[:, 1], label='-1', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        # Initialize the modules we need to build the network\n",
    "        self.linear1 = nn.Linear(num_inputs, 3, bias=True)\n",
    "        self.act_fc1 = nn.Tanh() \n",
    "        self.linear2 = nn.Linear(3, num_outputs, bias=True)\n",
    "        self.act_fc2 = nn.Tanh()    \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        y = self.linear1(x)\n",
    "        y = self.act_fc1(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.act_fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L3NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, neurons_l1, neurons_l2, neurons_l3, output_size):\n",
    "        super(L3NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, neurons_l1)\n",
    "        self.layer2 = nn.Linear(neurons_l2, neurons_l2)\n",
    "        self.layer3 = nn.Linear(neurons_l3, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.layer1(x))\n",
    "        x = torch.tanh(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data, loss_function, optimizer, device):\n",
    "    \n",
    "    model.train()  # Calcula gradientes\n",
    "    \n",
    "    N_batches = len(data)  # Número de batches = N_patrones/N_patrones_x_batch\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    #==============================================================\n",
    "    for idx,(X,y) in enumerate(data):\n",
    "\n",
    "        #-----------------------------------------------------\n",
    "        # Convierto los datos en tensores diferenciables\n",
    "        #-----------------------------------------------------\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Se limpia el caché del optimizador\n",
    "        \n",
    "        #----------------\n",
    "        # Forward pass\n",
    "        #----------------\n",
    "        y_pred = model(X)\n",
    "\n",
    "        #----------------\n",
    "        # Compute Loss\n",
    "        #----------------\n",
    "        if (data.batch_size == 1):\n",
    "            loss = loss_function(y_pred.squeeze(), y.squeeze())\n",
    "        else:\n",
    "            loss = loss_function(y_pred.squeeze(), y)\n",
    "        \n",
    "        error += loss.item()\n",
    "        \n",
    "        #----------------\n",
    "        # Backward pass\n",
    "        #----------------\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #==============================================================\n",
    "    \n",
    "    error /= N_batches\n",
    "    \n",
    "    return error, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(model, data, loss_function, device):\n",
    "    \n",
    "    model.eval()  # Turn off Dropouts Layers, BatchNorm Layers etc\n",
    "    \n",
    "    N_batches = len(data)  # Número de batches = N_patrones/N_patrones_x_batch\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    Y = torch.tensor([])\n",
    "    Yp = torch.tensor([])\n",
    "    \n",
    "    #==============================================================\n",
    "    with torch.no_grad():  # Turn off gradients computation\n",
    "        \n",
    "        for idx,(X,y) in enumerate(data):\n",
    "\n",
    "            Y = torch.hstack( (Y, y.flatten()) )\n",
    "\n",
    "            #-----------------------------------------------------\n",
    "            # Convierto los datos en tensores diferenciables\n",
    "            #-----------------------------------------------------\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            #----------------\n",
    "            # Forward pass\n",
    "            #----------------\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            Yp = torch.hstack( (Yp, y_pred.flatten().cpu()) )\n",
    "\n",
    "            #----------------\n",
    "            # Compute Loss\n",
    "            #----------------\n",
    "            loss = loss_function(y_pred.squeeze(), y.squeeze())\n",
    "\n",
    "            error += loss.item()\n",
    "    #==============================================================\n",
    "    \n",
    "    error /= N_batches\n",
    "    \n",
    "    #------------------\n",
    "    \n",
    "    return error, Y, Yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize experiment\n",
    "datafile = str(root) + params[\"input_file\"]\n",
    "train_data, test_data = load_split_save_data(datafile , output_name= params[\"EXP_NAME\"])\n",
    "# data\n",
    "filename_train_data = str(root) + '/exp/' + params[\"EXP_NAME\"] + '/' + params[\"EXP_NAME\"]  + '_train_data.csv'\n",
    "filename_test_data = str(root) + '/exp/' + params[\"EXP_NAME\"] + '/' +params[\"EXP_NAME\"]  + '_test_data.csv'\n",
    "\n",
    "# Construimos los datasets para entrenamiento y validación\n",
    "trn = DATASET(filename_train_data)\n",
    "test = DATASET(filename_test_data)\n",
    "\n",
    "# Construimos los dataloaders para entrenamiento y validación\n",
    "train_data = DataLoader(trn, batch_size=params[\"BATCH_SIZE\"], shuffle=True)\n",
    "validation_data = DataLoader(test, batch_size=params[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "# Inicializamos el modelo\n",
    "modelo = SimpleNN(num_inputs=2, num_outputs=1)\n",
    "#modelo = L3NN(2, 64, 64, 64, 2)\n",
    "modelo.to(device)\n",
    "\n",
    "# Definimos la función de LOSS a utilizar\n",
    "loss_function = nn.MSELoss(reduction='mean').to(device)\n",
    "#loss_function = nn.BCELoss().to(device)\n",
    "#loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "#loss_function = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Definimos el optimizador a utilizar\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=params[\"learning_rate\"], momentum=params[\"momentum\"])  # 0.9)\n",
    "#optimizer = optim.Adam(modelo.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9461139227662768,\n",
       " SimpleNN(\n",
       "   (linear1): Linear(in_features=2, out_features=3, bias=True)\n",
       "   (act_fc1): Tanh()\n",
       "   (linear2): Linear(in_features=3, out_features=1, bias=True)\n",
       "   (act_fc2): Tanh()\n",
       " ))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(modelo, train_data, loss_function, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 10 -- Error: 0.9721\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 20 -- Error: 0.9662\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 30 -- Error: 0.9699\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 40 -- Error: 0.9744\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 50 -- Error: 0.9691\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 60 -- Error: 0.9716\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 70 -- Error: 0.9652\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 80 -- Error: 0.961\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 90 -- Error: 0.9655\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 100 -- Error: 0.9721\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 110 -- Error: 0.962\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 120 -- Error: 0.9531\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 130 -- Error: 0.945\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 140 -- Error: 0.9376\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 150 -- Error: 0.9295\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 160 -- Error: 0.9243\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 170 -- Error: 0.9014\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 180 -- Error: 0.8831\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 190 -- Error: 0.8746\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 200 -- Error: 0.8283\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 210 -- Error: 0.8045\t--\tTasa acierto [train]: 0.5837320685386658\n",
      "\n",
      "Epoca: 220 -- Error: 0.7834\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 230 -- Error: 0.7554\t--\tTasa acierto [train]: 0.6698564887046814\n",
      "\n",
      "Epoca: 240 -- Error: 0.76\t--\tTasa acierto [train]: 0.6555023789405823\n",
      "\n",
      "Epoca: 250 -- Error: 0.7176\t--\tTasa acierto [train]: 0.6937798857688904\n",
      "\n",
      "Epoca: 260 -- Error: 0.7053\t--\tTasa acierto [train]: 0.6985645890235901\n",
      "\n",
      "Epoca: 270 -- Error: 0.7291\t--\tTasa acierto [train]: 0.6937798857688904\n",
      "\n",
      "Epoca: 280 -- Error: 0.7236\t--\tTasa acierto [train]: 0.6842105388641357\n",
      "\n",
      "Epoca: 290 -- Error: 0.7544\t--\tTasa acierto [train]: 0.6698564887046814\n",
      "\n",
      "Epoca: 300 -- Error: 0.6926\t--\tTasa acierto [train]: 0.7033492922782898\n",
      "\n",
      "Epoca: 310 -- Error: 0.7318\t--\tTasa acierto [train]: 0.6842105388641357\n",
      "\n",
      "Epoca: 320 -- Error: 0.7042\t--\tTasa acierto [train]: 0.6889952421188354\n",
      "\n",
      "Epoca: 330 -- Error: 0.7258\t--\tTasa acierto [train]: 0.6889952421188354\n",
      "\n",
      "Epoca: 340 -- Error: 0.685\t--\tTasa acierto [train]: 0.7129186391830444\n",
      "\n",
      "Epoca: 350 -- Error: 0.6721\t--\tTasa acierto [train]: 0.6985645890235901\n",
      "\n",
      "Epoca: 360 -- Error: 0.6492\t--\tTasa acierto [train]: 0.7224880456924438\n",
      "\n",
      "Epoca: 370 -- Error: 0.6571\t--\tTasa acierto [train]: 0.7224880456924438\n",
      "\n",
      "Epoca: 380 -- Error: 0.5579\t--\tTasa acierto [train]: 0.779904305934906\n",
      "\n",
      "Epoca: 390 -- Error: 0.4893\t--\tTasa acierto [train]: 0.8421052694320679\n",
      "\n",
      "Epoca: 400 -- Error: 0.3959\t--\tTasa acierto [train]: 0.8899521827697754\n",
      "\n",
      "Epoca: 410 -- Error: 0.3534\t--\tTasa acierto [train]: 0.9138755798339844\n",
      "\n",
      "Epoca: 420 -- Error: 0.295\t--\tTasa acierto [train]: 0.9665071964263916\n",
      "\n",
      "Epoca: 430 -- Error: 0.2734\t--\tTasa acierto [train]: 0.9521530866622925\n",
      "\n",
      "Epoca: 440 -- Error: 0.2496\t--\tTasa acierto [train]: 0.9952152967453003\n",
      "\n",
      "Epoca: 450 -- Error: 0.2327\t--\tTasa acierto [train]: 0.9952152967453003\n",
      "\n",
      "Epoca: 460 -- Error: 0.2215\t--\tTasa acierto [train]: 0.9952152967453003\n",
      "\n",
      "Epoca: 470 -- Error: 0.2131\t--\tTasa acierto [train]: 0.9617224931716919\n",
      "\n",
      "Epoca: 480 -- Error: 0.1979\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 490 -- Error: 0.1889\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 500 -- Error: 0.1825\t--\tTasa acierto [train]: 0.9952152967453003\n",
      "\n",
      "Epoca: 510 -- Error: 0.1815\t--\tTasa acierto [train]: 0.9665071964263916\n",
      "\n",
      "Epoca: 520 -- Error: 0.181\t--\tTasa acierto [train]: 0.9617224931716919\n",
      "\n",
      "Epoca: 530 -- Error: 0.1669\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 540 -- Error: 0.1605\t--\tTasa acierto [train]: 0.9952152967453003\n",
      "\n",
      "Epoca: 550 -- Error: 0.1567\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 560 -- Error: 0.1525\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 570 -- Error: 0.1493\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 580 -- Error: 0.1461\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 590 -- Error: 0.1431\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 600 -- Error: 0.1413\t--\tTasa acierto [train]: 0.9952152967453003\n",
      "\n",
      "Epoca: 610 -- Error: 0.1377\t--\tTasa acierto [train]: 0.9760765433311462\n",
      "\n",
      "Epoca: 620 -- Error: 0.1373\t--\tTasa acierto [train]: 0.980861246585846\n",
      "\n",
      "Epoca: 630 -- Error: 0.1347\t--\tTasa acierto [train]: 0.9665071964263916\n",
      "\n",
      "===============================================================================\n",
      "FINAL -- Epoca: 632 -- Error: 0.1367\t--\tTasa acierto [train]: 1.0\n",
      "===============================================================================\n",
      "Bias: [-2.7497318 -1.5629596  3.489052 ] -- W: [ 2.7039254  4.9418    -2.6396263  3.7872427 -4.682026  -0.5283551]\n"
     ]
    }
   ],
   "source": [
    "error = []   \n",
    "accuracy = []  \n",
    "STOP = False\n",
    "counter = 0\n",
    "best_model = None\n",
    "best_model_weights = None\n",
    "\n",
    "while (params[\"epoca\"] < params[\"MAX_EPOCAS\"]) and (params[\"acc\"] < params[\"MIN_ACC\"]) and (not STOP):\n",
    "\n",
    "    params[\"epoca\"] += 1\n",
    "    \n",
    "    # ENTRENAMIENTO    \n",
    "    _,modelo = train_step(modelo, train_data, loss_function, optimizer, device)\n",
    "    \n",
    "    # VALIDACION    \n",
    "    e,Y,Yp = predict_step(modelo, validation_data, loss_function, device)\n",
    "    params[\"acc\"] = torch.sum(Yp.sign() == Y.sign())/ len(Y)\n",
    "    \n",
    "    # ALMACENO MEDIDAS    \n",
    "    error.append(e)\n",
    "    accuracy.append(params[\"acc\"])   \n",
    "    \n",
    "    # CRITERIO DE CORTE Y ALMACENAMIENTO DEL MODELO   \n",
    "    if (e < params[\"MIN_ERROR\"]):\n",
    "        params[\"MIN_ERROR\"] = e\n",
    "        counter = 0\n",
    "        \n",
    "        \n",
    "        # Almaceno el modelo        \n",
    "        best_model = deepcopy(modelo)  # Genero una copia independiente\n",
    "        best_model_weights = best_model.state_dict()\n",
    "        \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter > params[\"MAX_COUNTER\"]:\n",
    "            STOP = True\n",
    "    \n",
    "    \n",
    "    # MUESTRO REPORTE POR PANTALLA (POR EPOCA)    \n",
    "    if (params[\"epoca\"] % 10) == 0:\n",
    "        print('Epoca: {} -- Error: {:.4}\\t--\\tTasa acierto [train]: {}\\n'.format(params[\"epoca\"], e, params[\"acc\"]))\n",
    "\n",
    "# MUESTRO REPORTE POR PANTALLA (FINAL)\n",
    "print('='*79)\n",
    "print('FINAL -- Epoca: {} -- Error: {:.4}\\t--\\tTasa acierto [train]: {}'.format(params[\"epoca\"], e, params[\"acc\"]))\n",
    "print('='*79)\n",
    "\n",
    "# GUARDO MEJOR MODELO A DISCO\n",
    "path_best_m = str(root) + '/exp/' + params[\"EXP_NAME\"] + '/' + params[\"EXP_NAME\"]  + 'best_model.pt'\n",
    "torch.save(best_model,\n",
    "           path_best_m,\n",
    "           _use_new_zipfile_serialization=True)        \n",
    "\n",
    "# GUARDAMOS LOS PESOS DEL MEJOR MODELO A DISCO\n",
    "path_best_m_state_dict = str(root) + '/exp/' + params[\"EXP_NAME\"] + '/' + params[\"EXP_NAME\"]  + 'best_model_state_dict.pt'\n",
    "torch.save(best_model.state_dict(),\n",
    "           path_best_m_state_dict,\n",
    "           _use_new_zipfile_serialization=True)\n",
    "\n",
    "B = best_model.linear1.bias.detach().cpu().numpy()\n",
    "W = best_model.linear1.weight.flatten().detach().cpu().numpy()\n",
    "print(f'Bias: {B} -- W: {W}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Import tensorboard logger from PyTorch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Load tensorboard extension for Jupyter Notebook, only need to start TB in the notebook\n",
    "%load_ext tensorboard\n",
    "# logging dir\n",
    "loggingdir = str(root) + '/exp/' + params[\"EXP_NAME\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_logger(model, optimizer, data_loader, loss_module, val_dataset, num_epochs=params[\"MAX_EPOCAS\"] , logging_dir=loggingdir):\n",
    "    # Create TensorBoard logger\n",
    "    writer = SummaryWriter(logging_dir)\n",
    "    model_plotted = False\n",
    "    \n",
    "    # Set model to train mode\n",
    "    model.train() \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_loss = 0.0\n",
    "        for X, y in data_loader:\n",
    "            \n",
    "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "            data_inputs = X.to(device)\n",
    "            data_labels = y.to(device)\n",
    "            \n",
    "            # For the very first batch, we visualize the computation graph in TensorBoard\n",
    "            if not model_plotted:\n",
    "                writer.add_graph(model, data_inputs)\n",
    "                model_plotted = True\n",
    "            \n",
    "            ## Step 2: Run the model on the input data\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            \n",
    "            ## Step 3: Calculate the loss\n",
    "            loss = loss_module(preds, data_labels.float())\n",
    "            \n",
    "            ## Step 4: Perform backpropagation\n",
    "            # Before calculating the gradients, we need to ensure that they are all zero. \n",
    "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "            optimizer.zero_grad() \n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            ## Step 5: Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            ## Step 6: Take the running average of the loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        # Add average loss to TensorBoard\n",
    "        epoch_loss /= len(data_loader)\n",
    "        writer.add_scalar('training_loss',\n",
    "                          epoch_loss,\n",
    "                          global_step = epoch + 1)\n",
    "        \n",
    "        # Visualize prediction and add figure to TensorBoard\n",
    "        # Since matplotlib figures can be slow in rendering, we only do it every 10th epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            fig = visualize_classification(model, val_dataset.x, val_dataset.y)\n",
    "            writer.add_figure('predictions',\n",
    "                              fig,\n",
    "                              global_step = epoch + 1)\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\n",
    "def visualize_classification(model, data, label):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.cpu().numpy()\n",
    "    if isinstance(label, torch.Tensor):\n",
    "        label = label.cpu().numpy()\n",
    "    data_0 = data[label == 0]\n",
    "    data_1 = data[label == 1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(4,4), dpi=500)\n",
    "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
    "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
    "    plt.title(\"Dataset samples\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Let's make use of a lot of operations we have learned above\n",
    "    model.to(device)\n",
    "    c0 = torch.Tensor(to_rgba(\"C0\")).to(device)\n",
    "    c1 = torch.Tensor(to_rgba(\"C1\")).to(device)\n",
    "    x1 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
    "    x2 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
    "    xx1, xx2 = torch.meshgrid(x1, x2, indexing='ij')  # Meshgrid function as in numpy\n",
    "    model_inputs = torch.stack([xx1, xx2], dim=-1)\n",
    "    preds = model(model_inputs)\n",
    "    preds = torch.sigmoid(preds)\n",
    "    output_image = (1 - preds) * c0[None,None] + preds * c1[None,None]  # Specifying \"None\" in a dimension creates a new one\n",
    "    output_image = output_image.cpu().numpy()  # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n",
    "    plt.imshow(output_image, origin='lower', extent=(-0.5, 1.5, -0.5, 1.5))\n",
    "    plt.grid(False)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729a7ac2409a44f0a63f5fadfdc4c9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model_with_logger(modelo, optimizer, train_data, loss_function, val_dataset=test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".neuralnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
