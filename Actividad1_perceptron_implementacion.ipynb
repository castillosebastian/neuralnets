{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import polars as pl\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cpu'\n",
    "sys.path.append('/home/sebacastillo/neuralnets/')\n",
    "from src.utils import get_project_root\n",
    "root = get_project_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OR_problem = False\n",
    "if OR_problem:\n",
    "   df = np.loadtxt(str(root) + '/data/OR.csv',\n",
    "                 delimiter=\",\")\n",
    "else:\n",
    "   df = np.loadtxt(str(root) + '/data/XOR.csv',\n",
    "                 delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentron original (Rosenblatt: 1962)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografía y fuentes:   \n",
    "- **Introduction to Machine Learning, MIT**:    \n",
    "  - [Perceptron](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week2/perceptron/?activate_block_id=block-v1%3AMITx%2B6.036%2B1T2019%2Btype%40sequential%2Bblock%40perceptron)\n",
    "  - [Neural Networks](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week6/neural_networks/?activate_block_id=block-v1%3AMITx%2B6.036%2B1T2019%2Btype%40sequential%2Bblock%40neural_networks)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(df)\n",
    "\n",
    "def ttsplit(data, prop=0.2):\n",
    "    assert isinstance(data, np.ndarray), \"Input should be a NumPy array\"\n",
    "    p = int(len(data) * prop)\n",
    "    test = data[:p, :]\n",
    "    train = data[p:, :]\n",
    "    X_train = train[:,:-1]\n",
    "    y_train = train[:,-1]\n",
    "    X_test = test[:,:-1]\n",
    "    y_test = test[:,-1]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = ttsplit(df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (theta): [-0.9025  -0.97479]\n",
      "Bias (theta_0): 1.0\n"
     ]
    }
   ],
   "source": [
    "# Perceptron\n",
    "input_data = X_train\n",
    "true_labels = y_train\n",
    "\n",
    "# Initialize the weights and bias\n",
    "n = input_data.shape[0]\n",
    "weights = np.ones(input_data.shape[1])*0.5\n",
    "bias = 0\n",
    "\n",
    "# Number of epochs (tau)\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i in range(n):\n",
    "        \n",
    "        x = input_data[i]\n",
    "        y_true = true_labels[i]\n",
    "\n",
    "        # Calculate the output using weights and bias. These is our hipothesis.        \n",
    "        # the sign mater for the next conditional product.\n",
    "        # The first output is always wrong! because all weight are set to 0\n",
    "        output = np.dot(weights, x) + bias\n",
    "\n",
    "        # Sign Function part where we compare the sign of the true label and the sign of our prediction.\n",
    "        # If both y_true and output have the same sign the result is positive and the condition fail.\n",
    "        # If it never fail it means the we hold the hipotesis (weight and bias).\n",
    "        # If both have diferent sign the result of the product is negative and the codition is true.\n",
    "        # So we should update the weights.\n",
    "        if y_true * output <= 0:\n",
    "            # Here is where learning happens as weight update\n",
    "            weights += y_true * x\n",
    "            bias += y_true\n",
    "\n",
    "        # Pendiente Loss Function: \n",
    "        # Crosentropy: -(np.sum(Y * np.log(A) + (1.0 - Y) * np.log(1.0 - A))) / m\n",
    "        # L2 (Error cuadrático): error = (y_true - y_pred) ** 2\n",
    "        # retroprogacación SGD.\n",
    "        \n",
    "\n",
    "# The final weights (theta) and bias (theta_0)\n",
    "print(\"Weights (theta):\", weights)\n",
    "print(\"Bias (theta_0):\", bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp.1:Acuracy: .76\n",
    "#Weights (theta): [1.98073 1.32935]\n",
    "#Bias (theta_0): 1.0\n",
    "# Exp.2: Acuracy: .73\n",
    "#Weights (theta): [-0.17513 -1.07761]\n",
    "#Bias (theta_0): 1.0\n",
    "# Exp.3: Acuracy: .71\n",
    "#Weights (theta): [1.39779 1.20869]\n",
    "#Bias (theta_0): 1.0\n",
    "# Exp.4 con 200 epoch: Acuracy: 51: Bajo!\n",
    "#Weights (theta): [-0.37849 -0.58772]\n",
    "#Bias (theta_0): 0.0\n",
    "# muchos con bajo accuracy\n",
    "# Exp.5: Accuracy .74\n",
    "#Weights (theta): [ 1.5557  -1.07645]\n",
    "#Bias (theta_0): -1.0\n",
    "# Exp.6: Accuracy .79\n",
    "#Weights (theta): [-1.88712  1.22394]\n",
    "#Bias (theta_0): -1.0\n",
    "# Exp.6: Accuracy .75\n",
    "#Weights (theta): [-0.9025  -0.97479]\n",
    "#Bias (theta_0): 1.0\n",
    "\n",
    "def validate_perceptron(X_test, y_test, weights, bias):\n",
    "    total_error = 0\n",
    "    correct_predictions = 0\n",
    "    n = X_test.shape[0]\n",
    "\n",
    "    for i in range(n):\n",
    "        x = X_test[i]\n",
    "        y_true = y_test[i]\n",
    "\n",
    "        # Calculate the output using weights and bias\n",
    "        output = np.dot(weights, x) + bias\n",
    "        #The sign function returns 1 if the input is positive, -1 if the input is negative, \n",
    "        # and 0 if the input is exactly 0. However, in the perceptron algorithm, we want to \n",
    "        # avoid having a prediction of 0, so we use a ternary expression to return 1 when the output is 0.\n",
    "        y_pred = np.sign(output) if output != 0 else 1\n",
    "\n",
    "        # Calculate the L2 loss\n",
    "        error = (y_true - y_pred) ** 2\n",
    "        total_error += error\n",
    "\n",
    "        # Update the accuracy counter\n",
    "        if y_pred == y_true:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # Calculate average error and accuracy\n",
    "    average_error = total_error / n\n",
    "    accuracy = correct_predictions / n\n",
    "\n",
    "    return average_error, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático (L2) en Test: 1.0\n",
      "Accuracy en Test: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Validate the perceptron model\n",
    "average_error, accuracy = validate_perceptron(X_test, y_test, weights, bias)\n",
    "\n",
    "print(\"Error cuadrático (L2) en Test:\", average_error)\n",
    "print(\"Accuracy en Test:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".neuralnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
