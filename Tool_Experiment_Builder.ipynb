{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full NN Build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup general del experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "acc = 0.  \n",
    "epoca = 0 \n",
    "input_file = '/data/concentlite.csv'\n",
    "EXP_NAME = 'EXP006'\n",
    "MIN_ACC = 1.0       # Defino mínimo accuracy deseado\n",
    "MIN_ERROR = 1E6     # Inicializo la variable para registrar el mínimo error cometido.\n",
    "MAX_EPOCAS = 1000    # Defino el número máximo de épocas de entrenamiento.\n",
    "MAX_COUNTER = 50   # Defino el máximo número de épocas  sin mejorar el error de validación stop train\n",
    "BATCH_SIZE = 10     # Número de patrones en cada batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "# NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "from matplotlib.colors import to_rgba\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "# Path\n",
    "import sys\n",
    "sys.path.append('/home/sebacastillo/neuralnets/')\n",
    "from src.utils import get_project_root\n",
    "root = get_project_root()\n",
    "## Check torch version\n",
    "print(f'Using {torch.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else torch.device('cpu'))\n",
    "torch.manual_seed(42)\n",
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_save_data(input_filename, output_name='EXP', split_type='train_test', train_ratio=0.75, validate_ratio=None, test_ratio=None):\n",
    "\n",
    "    data = pd.read_csv(input_filename)\n",
    "\n",
    "    # Check if 'exp' folder exists, create it if it doesn't\n",
    "    if not os.path.exists('exp'):\n",
    "        os.makedirs('exp')\n",
    "    \n",
    "    # Create a subfolder with the output_name\n",
    "    output_path = os.path.join('exp', output_name)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    if split_type == 'train_validate_test':\n",
    "        if not validate_ratio or not test_ratio:\n",
    "            raise ValueError(\"Please provide validate_ratio and test_ratio for 'train_validate_test' split type.\")\n",
    "        \n",
    "        train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "        validate_data, test_data = train_test_split(temp_data, train_size=validate_ratio / (validate_ratio + test_ratio), random_state=42)\n",
    "        \n",
    "        # Save the train, validate, and test data as CSV files in the output folder\n",
    "        train_data.to_csv(os.path.join(output_path, f'{output_name}_train_data.csv'), index=False)\n",
    "        validate_data.to_csv(os.path.join(output_path, f'{output_name}_validate_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(output_path, f'{output_name}_test_data.csv'), index=False)\n",
    "\n",
    "\n",
    "        return train_data, validate_data, test_data    \n",
    "\n",
    "    elif split_type == 'train_test':\n",
    "        train_data, test_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "        \n",
    "        # Save the train and test data as CSV files in the output folder\n",
    "        train_data.to_csv(os.path.join(output_path, f'{output_name}_train_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(output_path, f'{output_name}_test_data.csv'), index=False)\n",
    "\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid split_type. Use either 'train_validate_test' or 'train_test'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATASET(Dataset):  \n",
    "    '''\n",
    "    Esta clase maneja la lectura de los datos y provee un mecanismo\n",
    "    para alimentar los modelos con los patrones.\n",
    "    '''\n",
    "    \n",
    "    #===================================================\n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        #------------------------------------\n",
    "        # LECTURA DE LOS DATOS\n",
    "        data = pd.read_csv(filename, header=None).to_numpy() # Levanta los datos en formato numpy\n",
    "        \n",
    "        #------------------------------------\n",
    "        # INSERTAMOS COLUMNA DEL \"BIAS\"\n",
    "        #bias = -np.ones((len(data), 1))\n",
    "        #data = np.concatenate((bias, data), axis=1)  # Insertamos el \"bias\" en la primera columna\n",
    "        \n",
    "        #------------------------------------\n",
    "        # ALEATORIZO LOS PATRONES (filas)\n",
    "        idxs = np.arange(len(data))  # Genero un vector de índices\n",
    "        np.random.shuffle(idxs)\n",
    "        data = data[idxs,:]\n",
    "        \n",
    "        #------------------------------------\n",
    "        # SEPARO LOS DATOS\n",
    "        self.x = data[:,:-1].astype(np.float32)\n",
    "        self.y = data[:,-1].astype(np.float32)  # La clase está en la última columna\n",
    "    \n",
    "    #===================================================\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Devuelve el número de patrones en el dataset.\n",
    "        '''\n",
    "        return len(self.x)\n",
    "    \n",
    "    \n",
    "    #===================================================\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Devuelve el/los patrones indicados.\n",
    "        '''\n",
    "        return self.x[idx,:], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_with_labels(data):\n",
    "    # Filter data by label\n",
    "    data_label_1 = data[data[:, -1] == 1][:, 0:2]\n",
    "    data_label_minus_1 = data[data[:, -1] == -1][:, 0:2]\n",
    "\n",
    "    # Create scatter plots for each label\n",
    "    plt.scatter(data_label_1[:, 0], data_label_1[:, 1], label='1', alpha=0.5)\n",
    "    plt.scatter(data_label_minus_1[:, 0], data_label_minus_1[:, 1], label='-1', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        # Initialize the modules we need to build the network\n",
    "        self.linear1 = nn.Linear(num_inputs, 3, bias=True)\n",
    "        self.act_fc1 = nn.Tanh() \n",
    "        self.linear2 = nn.Linear(3, num_outputs, bias=True)\n",
    "        self.act_fc2 = nn.Tanh()    \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        y = self.linear1(x)\n",
    "        y = self.act_fc1(y)\n",
    "        y = self.linear2(y)\n",
    "        y = self.act_fc2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L3NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, neurons_l1, neurons_l2, neurons_l3, output_size):\n",
    "        super(L3NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, neurons_l1)\n",
    "        self.layer2 = nn.Linear(neurons_l2, neurons_l2)\n",
    "        self.layer3 = nn.Linear(neurons_l3, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.layer1(x))\n",
    "        x = torch.tanh(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data, loss_function, optimizer, device):\n",
    "    \n",
    "    model.train()  # Calcula gradientes\n",
    "    \n",
    "    N_batches = len(data)  # Número de batches = N_patrones/N_patrones_x_batch\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    #==============================================================\n",
    "    for idx,(X,y) in enumerate(data):\n",
    "\n",
    "        #-----------------------------------------------------\n",
    "        # Convierto los datos en tensores diferenciables\n",
    "        #-----------------------------------------------------\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Se limpia el caché del optimizador\n",
    "        \n",
    "        #----------------\n",
    "        # Forward pass\n",
    "        #----------------\n",
    "        y_pred = model(X)\n",
    "\n",
    "        #----------------\n",
    "        # Compute Loss\n",
    "        #----------------\n",
    "        if (data.batch_size == 1):\n",
    "            loss = loss_function(y_pred.squeeze(), y.squeeze())\n",
    "        else:\n",
    "            loss = loss_function(y_pred.squeeze(), y)\n",
    "        \n",
    "        error += loss.item()\n",
    "        \n",
    "        #----------------\n",
    "        # Backward pass\n",
    "        #----------------\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #==============================================================\n",
    "    \n",
    "    error /= N_batches\n",
    "    \n",
    "    return error, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(model, data, loss_function, device):\n",
    "    \n",
    "    model.eval()  # Turn off Dropouts Layers, BatchNorm Layers etc\n",
    "    \n",
    "    N_batches = len(data)  # Número de batches = N_patrones/N_patrones_x_batch\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    Y = torch.tensor([])\n",
    "    Yp = torch.tensor([])\n",
    "    \n",
    "    #==============================================================\n",
    "    with torch.no_grad():  # Turn off gradients computation\n",
    "        \n",
    "        for idx,(X,y) in enumerate(data):\n",
    "\n",
    "            Y = torch.hstack( (Y, y.flatten()) )\n",
    "\n",
    "            #-----------------------------------------------------\n",
    "            # Convierto los datos en tensores diferenciables\n",
    "            #-----------------------------------------------------\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            #----------------\n",
    "            # Forward pass\n",
    "            #----------------\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            Yp = torch.hstack( (Yp, y_pred.flatten().cpu()) )\n",
    "\n",
    "            #----------------\n",
    "            # Compute Loss\n",
    "            #----------------\n",
    "            loss = loss_function(y_pred.squeeze(), y.squeeze())\n",
    "\n",
    "            error += loss.item()\n",
    "    #==============================================================\n",
    "    \n",
    "    error /= N_batches\n",
    "    \n",
    "    #------------------\n",
    "    \n",
    "    return error, Y, Yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialize experiment\n",
    "datafile = str(root) + input_file\n",
    "train_data, test_data = load_split_save_data(datafile , output_name= EXP_NAME)\n",
    "# data\n",
    "filename_train_data = str(root) + '/exp/' + EXP_NAME + '/' + EXP_NAME  + '_train_data.csv'\n",
    "filename_test_data = str(root) + '/exp/' + EXP_NAME + '/' + EXP_NAME  + '_test_data.csv'\n",
    "\n",
    "# Construimos los datasets para entrenamiento y validación\n",
    "trn = DATASET(filename_train_data)\n",
    "test = DATASET(filename_test_data)\n",
    "\n",
    "# Construimos los dataloaders para entrenamiento y validación\n",
    "train_data = DataLoader(trn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_data = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Inicializamos el modelo\n",
    "modelo = SimpleNN(num_inputs=2, num_outputs=1)\n",
    "#modelo = L3NN(2, 64, 64, 64, 2)\n",
    "modelo.to(device)\n",
    "\n",
    "# Definimos la función de LOSS a utilizar\n",
    "loss_function = nn.MSELoss(reduction='mean').to(device)\n",
    "#loss_function = nn.BCELoss().to(device)\n",
    "#loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "#loss_function = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Definimos el optimizador a utilizar\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=learning_rate, momentum=0.9)  # 0.9)\n",
    "#optimizer = optim.Adam(modelo.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_step(modelo, train_data, loss_function, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 10 -- Error: 0.9731\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 20 -- Error: 0.9738\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 30 -- Error: 0.9605\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 40 -- Error: 1.012\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 50 -- Error: 0.9297\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 60 -- Error: 0.8934\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 70 -- Error: 0.8214\t--\tTasa acierto [train]: 0.5693780183792114\n",
      "\n",
      "Epoca: 80 -- Error: 0.7564\t--\tTasa acierto [train]: 0.679425835609436\n",
      "\n",
      "Epoca: 90 -- Error: 0.7106\t--\tTasa acierto [train]: 0.6985645890235901\n",
      "\n",
      "Epoca: 100 -- Error: 0.7485\t--\tTasa acierto [train]: 0.6746411323547363\n",
      "\n",
      "Epoca: 110 -- Error: 0.6985\t--\tTasa acierto [train]: 0.7129186391830444\n",
      "\n",
      "Epoca: 120 -- Error: 0.7124\t--\tTasa acierto [train]: 0.6842105388641357\n",
      "\n",
      "Epoca: 130 -- Error: 0.697\t--\tTasa acierto [train]: 0.6889952421188354\n",
      "\n",
      "Epoca: 140 -- Error: 0.6778\t--\tTasa acierto [train]: 0.7129186391830444\n",
      "\n",
      "Epoca: 150 -- Error: 0.7654\t--\tTasa acierto [train]: 0.679425835609436\n",
      "\n",
      "Epoca: 160 -- Error: 0.6795\t--\tTasa acierto [train]: 0.6985645890235901\n",
      "\n",
      "Epoca: 170 -- Error: 0.7567\t--\tTasa acierto [train]: 0.660287082195282\n",
      "\n",
      "Epoca: 180 -- Error: 0.6974\t--\tTasa acierto [train]: 0.6889952421188354\n",
      "\n",
      "Epoca: 190 -- Error: 0.6994\t--\tTasa acierto [train]: 0.7033492922782898\n",
      "\n",
      "Epoca: 200 -- Error: 0.6725\t--\tTasa acierto [train]: 0.7081339955329895\n",
      "\n",
      "Epoca: 210 -- Error: 0.6871\t--\tTasa acierto [train]: 0.7033492922782898\n",
      "\n",
      "Epoca: 220 -- Error: 0.6893\t--\tTasa acierto [train]: 0.7033492922782898\n",
      "\n",
      "Epoca: 230 -- Error: 0.6866\t--\tTasa acierto [train]: 0.7033492922782898\n",
      "\n",
      "Epoca: 240 -- Error: 0.6814\t--\tTasa acierto [train]: 0.7081339955329895\n",
      "\n",
      "Epoca: 250 -- Error: 0.6697\t--\tTasa acierto [train]: 0.7272727489471436\n",
      "\n",
      "Epoca: 260 -- Error: 0.717\t--\tTasa acierto [train]: 0.6746411323547363\n",
      "\n",
      "Epoca: 270 -- Error: 0.6687\t--\tTasa acierto [train]: 0.7416267991065979\n",
      "\n",
      "Epoca: 280 -- Error: 0.7319\t--\tTasa acierto [train]: 0.6746411323547363\n",
      "\n",
      "Epoca: 290 -- Error: 0.6762\t--\tTasa acierto [train]: 0.6985645890235901\n",
      "\n",
      "Epoca: 300 -- Error: 0.6844\t--\tTasa acierto [train]: 0.6985645890235901\n",
      "\n",
      "Epoca: 310 -- Error: 0.6729\t--\tTasa acierto [train]: 0.7320573925971985\n",
      "\n",
      "Epoca: 320 -- Error: 0.6785\t--\tTasa acierto [train]: 0.6985645890235901\n",
      "\n",
      "===============================================================================\n",
      "FINAL -- Epoca: 321 -- Error: 0.704\t--\tTasa acierto [train]: 0.6889952421188354\n",
      "===============================================================================\n",
      "Bias: [-1.6769032   0.28881165  3.2656798 ] -- W: [ 1.9010551   4.2159576   0.50124675  1.171662   -1.2970002  -2.8528476 ]\n"
     ]
    }
   ],
   "source": [
    "error = []   \n",
    "accuracy = []  \n",
    "STOP = False\n",
    "counter = 0\n",
    "best_model = None\n",
    "best_model_weights = None\n",
    "\n",
    "while (epoca < MAX_EPOCAS) and (acc < MIN_ACC) and (not STOP):\n",
    "\n",
    "    epoca += 1\n",
    "    \n",
    "    # ENTRENAMIENTO    \n",
    "    _,modelo = train_step(modelo, train_data, loss_function, optimizer, device)\n",
    "    \n",
    "    # VALIDACION    \n",
    "    e,Y,Yp = predict_step(modelo, validation_data, loss_function, device)\n",
    "    acc = torch.sum(Yp.sign() == Y.sign())/ len(Y)\n",
    "    \n",
    "    # ALMACENO MEDIDAS    \n",
    "    error.append(e)\n",
    "    accuracy.append(acc)   \n",
    "    \n",
    "    # CRITERIO DE CORTE Y ALMACENAMIENTO DEL MODELO   \n",
    "    if (e < MIN_ERROR):\n",
    "        MIN_ERROR = e\n",
    "        counter = 0\n",
    "        \n",
    "        \n",
    "        # Almaceno el modelo        \n",
    "        best_model = deepcopy(modelo)  # Genero una copia independiente\n",
    "        best_model_weights = best_model.state_dict()\n",
    "        \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter > MAX_COUNTER:\n",
    "            STOP = True\n",
    "    \n",
    "    \n",
    "    # MUESTRO REPORTE POR PANTALLA (POR EPOCA)    \n",
    "    if (epoca % 10) == 0:\n",
    "        print('Epoca: {} -- Error: {:.4}\\t--\\tTasa acierto [train]: {}\\n'.format(epoca, e, acc))\n",
    "\n",
    "# MUESTRO REPORTE POR PANTALLA (FINAL)\n",
    "print('='*79)\n",
    "print('FINAL -- Epoca: {} -- Error: {:.4}\\t--\\tTasa acierto [train]: {}'.format(epoca, e, acc))\n",
    "print('='*79)\n",
    "\n",
    "# GUARDO MEJOR MODELO A DISCO\n",
    "path_best_m = str(root) + '/exp/' + EXP_NAME + '/' + EXP_NAME  + 'best_model.pt'\n",
    "torch.save(best_model,\n",
    "           path_best_m,\n",
    "           _use_new_zipfile_serialization=True)        \n",
    "\n",
    "# GUARDAMOS LOS PESOS DEL MEJOR MODELO A DISCO\n",
    "path_best_m_state_dict = str(root) + '/exp/' + EXP_NAME + '/' + EXP_NAME  + 'best_model_state_dict.pt'\n",
    "torch.save(best_model.state_dict(),\n",
    "           path_best_m_state_dict,\n",
    "           _use_new_zipfile_serialization=True)\n",
    "\n",
    "B = best_model.linear1.bias.detach().cpu().numpy()\n",
    "W = best_model.linear1.weight.flatten().detach().cpu().numpy()\n",
    "print(f'Bias: {B} -- W: {W}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Import tensorboard logger from PyTorch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Load tensorboard extension for Jupyter Notebook, only need to start TB in the notebook\n",
    "%load_ext tensorboard\n",
    "# logging dir\n",
    "loggingdir = str(root) + '/exp/' + EXP_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_logger(model, optimizer, data_loader, loss_module, val_dataset, num_epochs=MAX_EPOCAS, logging_dir=loggingdir):\n",
    "    # Create TensorBoard logger\n",
    "    writer = SummaryWriter(logging_dir)\n",
    "    model_plotted = False\n",
    "    \n",
    "    # Set model to train mode\n",
    "    model.train() \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_loss = 0.0\n",
    "        for X, y in data_loader:\n",
    "            \n",
    "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "            data_inputs = X.to(device)\n",
    "            data_labels = y.to(device)\n",
    "            \n",
    "            # For the very first batch, we visualize the computation graph in TensorBoard\n",
    "            if not model_plotted:\n",
    "                writer.add_graph(model, data_inputs)\n",
    "                model_plotted = True\n",
    "            \n",
    "            ## Step 2: Run the model on the input data\n",
    "            preds = model(data_inputs)\n",
    "            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            \n",
    "            ## Step 3: Calculate the loss\n",
    "            loss = loss_module(preds, data_labels.float())\n",
    "            \n",
    "            ## Step 4: Perform backpropagation\n",
    "            # Before calculating the gradients, we need to ensure that they are all zero. \n",
    "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "            optimizer.zero_grad() \n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            ## Step 5: Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            ## Step 6: Take the running average of the loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        # Add average loss to TensorBoard\n",
    "        epoch_loss /= len(data_loader)\n",
    "        writer.add_scalar('training_loss',\n",
    "                          epoch_loss,\n",
    "                          global_step = epoch + 1)\n",
    "        \n",
    "        # Visualize prediction and add figure to TensorBoard\n",
    "        # Since matplotlib figures can be slow in rendering, we only do it every 10th epoch\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            fig = visualize_classification(model, val_dataset.x, val_dataset.y)\n",
    "            writer.add_figure('predictions',\n",
    "                              fig,\n",
    "                              global_step = epoch + 1)\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\n",
    "def visualize_classification(model, data, label):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        data = data.cpu().numpy()\n",
    "    if isinstance(label, torch.Tensor):\n",
    "        label = label.cpu().numpy()\n",
    "    data_0 = data[label == 0]\n",
    "    data_1 = data[label == 1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(4,4), dpi=500)\n",
    "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
    "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
    "    plt.title(\"Dataset samples\")\n",
    "    plt.ylabel(r\"$x_2$\")\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Let's make use of a lot of operations we have learned above\n",
    "    model.to(device)\n",
    "    c0 = torch.Tensor(to_rgba(\"C0\")).to(device)\n",
    "    c1 = torch.Tensor(to_rgba(\"C1\")).to(device)\n",
    "    x1 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
    "    x2 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
    "    xx1, xx2 = torch.meshgrid(x1, x2, indexing='ij')  # Meshgrid function as in numpy\n",
    "    model_inputs = torch.stack([xx1, xx2], dim=-1)\n",
    "    preds = model(model_inputs)\n",
    "    preds = torch.sigmoid(preds)\n",
    "    output_image = (1 - preds) * c0[None,None] + preds * c1[None,None]  # Specifying \"None\" in a dimension creates a new one\n",
    "    output_image = output_image.cpu().numpy()  # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n",
    "    plt.imshow(output_image, origin='lower', extent=(-0.5, 1.5, -0.5, 1.5))\n",
    "    plt.grid(False)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadfc64ca6aa4f78ba64765d567a48b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model_with_logger(modelo, optimizer, train_data, loss_function, val_dataset=test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".neuralnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
