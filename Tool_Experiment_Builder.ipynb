{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full NN Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "# NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Ploting\n",
    "import matplotlib.pyplot as plt\n",
    "# Path\n",
    "import sys\n",
    "sys.path.append('/home/sebacastillo/neuralnets/')\n",
    "from src.utils import get_project_root\n",
    "root = get_project_root()\n",
    "## Check torch version\n",
    "print(f'Using {torch.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else torch.device('cpu'))\n",
    "torch.manual_seed(42)\n",
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_save_data(input_filename, output_name='EXP', split_type='train_test', train_ratio=0.75, validate_ratio=None, test_ratio=None):\n",
    "\n",
    "    data = pd.read_csv(input_filename)\n",
    "\n",
    "    # Check if 'exp' folder exists, create it if it doesn't\n",
    "    if not os.path.exists('exp'):\n",
    "        os.makedirs('exp')\n",
    "    \n",
    "    # Create a subfolder with the output_name\n",
    "    output_path = os.path.join('exp', output_name)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    if split_type == 'train_validate_test':\n",
    "        if not validate_ratio or not test_ratio:\n",
    "            raise ValueError(\"Please provide validate_ratio and test_ratio for 'train_validate_test' split type.\")\n",
    "        \n",
    "        train_data, temp_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "        validate_data, test_data = train_test_split(temp_data, train_size=validate_ratio / (validate_ratio + test_ratio), random_state=42)\n",
    "        \n",
    "        # Save the train, validate, and test data as CSV files in the output folder\n",
    "        train_data.to_csv(os.path.join(output_path, f'{output_name}_train_data.csv'), index=False)\n",
    "        validate_data.to_csv(os.path.join(output_path, f'{output_name}_validate_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(output_path, f'{output_name}_test_data.csv'), index=False)\n",
    "\n",
    "\n",
    "        return train_data, validate_data, test_data    \n",
    "\n",
    "    elif split_type == 'train_test':\n",
    "        train_data, test_data = train_test_split(data, train_size=train_ratio, random_state=42)\n",
    "        \n",
    "        # Save the train and test data as CSV files in the output folder\n",
    "        train_data.to_csv(os.path.join(output_path, f'{output_name}_train_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(output_path, f'{output_name}_test_data.csv'), index=False)\n",
    "\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid split_type. Use either 'train_validate_test' or 'train_test'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATASET(Dataset):  \n",
    "    '''\n",
    "    Esta clase maneja la lectura de los datos y provee un mecanismo\n",
    "    para alimentar los modelos con los patrones.\n",
    "    '''\n",
    "    \n",
    "    #===================================================\n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        #------------------------------------\n",
    "        # LECTURA DE LOS DATOS\n",
    "        data = pd.read_csv(filename, header=None).to_numpy() # Levanta los datos en formato numpy\n",
    "        \n",
    "        #------------------------------------\n",
    "        # INSERTAMOS COLUMNA DEL \"BIAS\"\n",
    "        #bias = -np.ones((len(data), 1))\n",
    "        #data = np.concatenate((bias, data), axis=1)  # Insertamos el \"bias\" en la primera columna\n",
    "        \n",
    "        #------------------------------------\n",
    "        # ALEATORIZO LOS PATRONES (filas)\n",
    "        idxs = np.arange(len(data))  # Genero un vector de índices\n",
    "        np.random.shuffle(idxs)\n",
    "        data = data[idxs,:]\n",
    "        \n",
    "        #------------------------------------\n",
    "        # SEPARO LOS DATOS\n",
    "        self.x = data[:,:-1].astype(np.float32)\n",
    "        self.y = data[:,-1].astype(np.float32)  # La clase está en la última columna\n",
    "    \n",
    "    #===================================================\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Devuelve el número de patrones en el dataset.\n",
    "        '''\n",
    "        return len(self.x)\n",
    "    \n",
    "    \n",
    "    #===================================================\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Devuelve el/los patrones indicados.\n",
    "        '''\n",
    "        return self.x[idx,:], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_with_labels(data):\n",
    "    # Filter data by label\n",
    "    data_label_1 = data[data[:, -1] == 1][:, 0:2]\n",
    "    data_label_minus_1 = data[data[:, -1] == -1][:, 0:2]\n",
    "\n",
    "    # Create scatter plots for each label\n",
    "    plt.scatter(data_label_1[:, 0], data_label_1[:, 1], label='1', alpha=0.5)\n",
    "    plt.scatter(data_label_minus_1[:, 0], data_label_minus_1[:, 1], label='-1', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super().__init__()\n",
    "        # Initialize the modules we need to build the network\n",
    "        self.linear1 = nn.Linear(num_inputs, num_outputs, bias=True)\n",
    "        self.act_fc = nn.Tanh()        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        y = self.linear1(x)\n",
    "        y = self.act_fc(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L3NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, neurons_l1, neurons_l2, neurons_l3, output_size):\n",
    "        super(L3NN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, neurons_l1)\n",
    "        self.layer2 = nn.Linear(neurons_l2, neurons_l2)\n",
    "        self.layer3 = nn.Linear(neurons_l3, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.layer1(x))\n",
    "        x = torch.tanh(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, data, loss_function, optimizer, device):\n",
    "    \n",
    "    model.train()  # Calcula gradientes\n",
    "    \n",
    "    N_batches = len(data)  # Número de batches = N_patrones/N_patrones_x_batch\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    #==============================================================\n",
    "    for idx,(X,y) in enumerate(data):\n",
    "\n",
    "        #-----------------------------------------------------\n",
    "        # Convierto los datos en tensores diferenciables\n",
    "        #-----------------------------------------------------\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Se limpia el caché del optimizador\n",
    "        \n",
    "        #----------------\n",
    "        # Forward pass\n",
    "        #----------------\n",
    "        y_pred = model(X)\n",
    "\n",
    "        #----------------\n",
    "        # Compute Loss\n",
    "        #----------------\n",
    "        if (data.batch_size == 1):\n",
    "            loss = loss_function(y_pred.squeeze(), y.squeeze())\n",
    "        else:\n",
    "            loss = loss_function(y_pred.squeeze(), y)\n",
    "        \n",
    "        error += loss.item()\n",
    "        \n",
    "        #----------------\n",
    "        # Backward pass\n",
    "        #----------------\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #==============================================================\n",
    "    \n",
    "    error /= N_batches\n",
    "    \n",
    "    return error, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(model, data, loss_function, device):\n",
    "    \n",
    "    model.eval()  # Turn off Dropouts Layers, BatchNorm Layers etc\n",
    "    \n",
    "    N_batches = len(data)  # Número de batches = N_patrones/N_patrones_x_batch\n",
    "    \n",
    "    error = 0\n",
    "    \n",
    "    Y = torch.tensor([])\n",
    "    Yp = torch.tensor([])\n",
    "    \n",
    "    #==============================================================\n",
    "    with torch.no_grad():  # Turn off gradients computation\n",
    "        \n",
    "        for idx,(X,y) in enumerate(data):\n",
    "\n",
    "            Y = torch.hstack( (Y, y.flatten()) )\n",
    "\n",
    "            #-----------------------------------------------------\n",
    "            # Convierto los datos en tensores diferenciables\n",
    "            #-----------------------------------------------------\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            #----------------\n",
    "            # Forward pass\n",
    "            #----------------\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            Yp = torch.hstack( (Yp, y_pred.flatten().cpu()) )\n",
    "\n",
    "            #----------------\n",
    "            # Compute Loss\n",
    "            #----------------\n",
    "            loss = loss_function(y_pred.squeeze(), y.squeeze())\n",
    "\n",
    "            error += loss.item()\n",
    "    #==============================================================\n",
    "    \n",
    "    error /= N_batches\n",
    "    \n",
    "    #------------------\n",
    "    \n",
    "    return error, Y, Yp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "acc = 0.  \n",
    "epoca = 0 \n",
    "input_file = '/data/concentlite.csv'\n",
    "EXP_NAME = 'EXP003'\n",
    "MIN_ACC = 1.0       # Defino mínimo accuracy deseado\n",
    "MIN_ERROR = 1E6     # Inicializo la variable para registrar el mínimo error cometido.\n",
    "MAX_EPOCAS = 500    # Defino el número máximo de épocas de entrenamiento.\n",
    "MAX_COUNTER = 50    # Defino el máximo número de épocas  sin mejorar el error de validación stop train\n",
    "BATCH_SIZE = 10     # Número de patrones en cada batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = str(root) + input_file\n",
    "train_data, test_data = load_split_save_data(datafile , output_name= EXP_NAME)\n",
    "# data\n",
    "filename_train_data = str(root) + '/exp/' + EXP_NAME + '/' + EXP_NAME  + '_train_data.csv'\n",
    "filename_test_data = str(root) + '/exp/' + EXP_NAME + '/' + EXP_NAME  + '_test_data.csv'\n",
    "\n",
    "# Construimos los datasets para entrenamiento y validación\n",
    "trn = DATASET(filename_train_data)\n",
    "test = DATASET(filename_test_data)\n",
    "\n",
    "# Construimos los dataloaders para entrenamiento y validación\n",
    "train_data = DataLoader(trn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_data = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Inicializamos el modelo\n",
    "modelo = SimpleNN(num_inputs=2, num_outputs=1)\n",
    "#modelo = L3NN(2, 64, 64, 64, 2)\n",
    "modelo.to(device)\n",
    "\n",
    "# Definimos la función de LOSS a utilizar\n",
    "loss_function = nn.MSELoss(reduction='mean').to(device)\n",
    "#loss_function = nn.BCELoss().to(device)\n",
    "#loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "#loss_function = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Definimos el optimizador a utilizar\n",
    "optimizer = optim.SGD(modelo.parameters(), lr=learning_rate, momentum=0.9)  # 0.9)\n",
    "#optimizer = optim.Adam(modelo.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x2 and 3x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_step(modelo, train_data, loss_function, optimizer, device)\n",
      "Cell \u001b[0;32mIn[74], line 23\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, data, loss_function, optimizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Se limpia el caché del optimizador\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m#----------------\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m#----------------\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m y_pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     25\u001b[0m \u001b[39m#----------------\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# Compute Loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m#----------------\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m (data\u001b[39m.\u001b[39mbatch_size \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m):\n",
      "File \u001b[0;32m~/.neuralnets/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[72], line 10\u001b[0m, in \u001b[0;36mSimpleNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m      9\u001b[0m     \u001b[39m# Perform the calculation of the model to determine the prediction\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(x)\n\u001b[1;32m     11\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fc(y)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/.neuralnets/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.neuralnets/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x2 and 3x1)"
     ]
    }
   ],
   "source": [
    "train_step(modelo, train_data, loss_function, optimizer, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca: 10 -- Error: 0.9829\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 20 -- Error: 0.9678\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 30 -- Error: 0.9582\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 40 -- Error: 0.9657\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "Epoca: 50 -- Error: 0.9583\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "\n",
      "===============================================================================\n",
      "FINAL -- Epoca: 53 -- Error: 0.9714\t--\tTasa acierto [train]: 0.6028708219528198\n",
      "===============================================================================\n",
      "Bias: [0.10051076] -- W: [0.23321992 0.02609202]\n"
     ]
    }
   ],
   "source": [
    "error = []   \n",
    "accuracy = []  \n",
    "STOP = False\n",
    "counter = 0\n",
    "best_model = None\n",
    "best_model_weights = None\n",
    "\n",
    "while (epoca < MAX_EPOCAS) and (acc < MIN_ACC) and (not STOP):\n",
    "\n",
    "    epoca += 1\n",
    "    \n",
    "    # ENTRENAMIENTO    \n",
    "    _,modelo = train_step(modelo, train_data, loss_function, optimizer, device)\n",
    "    \n",
    "    # VALIDACION    \n",
    "    e,Y,Yp = predict_step(modelo, validation_data, loss_function, device)\n",
    "    acc = torch.sum(Yp.sign() == Y.sign())/ len(Y)\n",
    "    \n",
    "    # ALMACENO MEDIDAS    \n",
    "    error.append(e)\n",
    "    accuracy.append(acc)   \n",
    "    \n",
    "    # CRITERIO DE CORTE Y ALMACENAMIENTO DEL MODELO   \n",
    "    if (e < MIN_ERROR):\n",
    "        MIN_ERROR = e\n",
    "        counter = 0\n",
    "        \n",
    "        \n",
    "        # Almaceno el modelo        \n",
    "        best_model = deepcopy(modelo)  # Genero una copia independiente\n",
    "        best_model_weights = best_model.state_dict()\n",
    "        \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter > MAX_COUNTER:\n",
    "            STOP = True\n",
    "    \n",
    "    \n",
    "    # MUESTRO REPORTE POR PANTALLA (POR EPOCA)    \n",
    "    if (epoca % 10) == 0:\n",
    "        print('Epoca: {} -- Error: {:.4}\\t--\\tTasa acierto [train]: {}\\n'.format(epoca, e, acc))\n",
    "\n",
    "# MUESTRO REPORTE POR PANTALLA (FINAL)\n",
    "print('='*79)\n",
    "print('FINAL -- Epoca: {} -- Error: {:.4}\\t--\\tTasa acierto [train]: {}'.format(epoca, e, acc))\n",
    "print('='*79)\n",
    "\n",
    "# GUARDO MEJOR MODELO A DISCO\n",
    "path_best_m = str(root) + '/exp/' + EXP_NAME + '/' + EXP_NAME  + 'best_model.pt'\n",
    "torch.save(best_model,\n",
    "           path_best_m,\n",
    "           _use_new_zipfile_serialization=True)        \n",
    "\n",
    "# GUARDAMOS LOS PESOS DEL MEJOR MODELO A DISCO\n",
    "path_best_m_state_dict = str(root) + '/exp/' + EXP_NAME + '/' + EXP_NAME  + 'best_model_state_dict.pt'\n",
    "torch.save(best_model.state_dict(),\n",
    "           path_best_m_state_dict,\n",
    "           _use_new_zipfile_serialization=True)\n",
    "\n",
    "B = best_model.linear1.bias.detach().cpu().numpy()\n",
    "W = best_model.linear1.weight.flatten().detach().cpu().numpy()\n",
    "print(f'Bias: {B} -- W: {W}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".neuralnets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
